{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "Large number of features in the dataset is one of the factors that affect both the training time as well as accuracy of machine learning models. You have different options to deal with huge number of features in a dataset.\n",
    "\n",
    "1. Try to train the models on original number of features, which take days or weeks if the number of features is too high.\n",
    "2. Reduce the number of variables by merging correlated variables.\n",
    "3. Extract the most important features from the dataset that are responsible for maximum variance in the output. Different statistical techniques are used for this purpose e.g. linear discriminant analysis, factor analysis, and principal component analysis.\n",
    "\n",
    "Principal component analysis, or PCA, is a statistical technique to convert high dimensional data to low dimensional data by selecting the most important features that capture maximum information about the dataset. The features are selected on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal component. The feature that is responsible for second highest variance is considered the second principal component, and so on. It is important to mention that principal components do not have any correlation with each other.\n",
    "\n",
    "### Steps to perform PCA\n",
    "1. Collect and clean data.\n",
    "2. Standardize data using the Scikit-learn StandardScaler() class.\n",
    "3. Create a PCA model (pca_1) with PCA(n_components=None) and train the model with standardized data. n_components=None means we keep all the components for now.\n",
    "3. Select the best number of principal components for the dataset by creating the plot discussed in Question 14.\n",
    "4. Create another PCA model (pca_2) with PCA(n_components=k) and train the model with standardized data. Here, k is the best number of principal components we selected earlier.\n",
    "5. Get the transformed dataset by using the pca_2.transform() method.\n",
    "\n",
    "### Advantages of PCA\n",
    "There are two main advantages of dimensionality reduction with PCA.\n",
    "\n",
    "The training time of the algorithms reduces significantly with less number of features.\n",
    "It is not always possible to analyze data in high dimensions. For instance if there are 100 features in a dataset. Total number of scatter plots required to visualize the data would be 100(100-1)2 = 4950. Practically it is not possible to analyze data this way.\n",
    "### Normalization of Features\n",
    "It is imperative to mention that a feature set must be normalized before applying PCA. For instance if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.\n",
    "\n",
    "Finally, the last point to remember before we start coding is that PCA is a statistical technique and can only be applied to numeric data. Therefore, categorical features are required to be converted into numerical features before PCA can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing dataset\n",
    "df = pd.read_csv('iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining features and labels\n",
    "X = df.iloc[: , :-1]\n",
    "y= df.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PCA performs best with a normalized feature set. We will perform standard scalar normalization to normalize our feature set. To do this, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA\n",
    "It is only a matter of three lines of code to perform PCA using Python's Scikit-Learn library. The PCA class is used for this purpose. PCA depends only upon the feature set and not the label data. Therefore, PCA can be considered as an unsupervised machine learning technique.\n",
    "\n",
    "Performing PCA using Scikit-Learn is a two-step process:\n",
    "\n",
    "1. Initialize the PCA class by passing the number of components to the constructor.\n",
    "2. Call the fit and then transform methods by passing the feature set to these methods. The transform method returns the specified number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train_p = pca.fit_transform(X_train)\n",
    "X_test_p  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we create a PCA object named pca. We did not specify the number of components in the constructor. Hence, all four of the features in the feature set will be returned for both the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained_variance_ratio_\n",
    "The PCA class contains explained_variance_ratio_ which returns the variance caused by each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72229951, 0.2397406 , 0.03335483, 0.00460506])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained_variance variable is now a float type array which contains variance ratios for each principal component.\n",
    "It can be seen that first principal component is responsible for 72.22% variance. Similarly, the second principal component causes 23.9% variance in the dataset. Collectively we can say that (72.22 + 23.9) 96.21% percent of the classification information contained in the feature set is captured by the first two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Select the Best Number of Principal Components for the Dataset\n",
    "\n",
    "Plot the explained variance percentage of individual components and the percentage of total variance captured by all principal components.\n",
    "\n",
    "This is the most advanced and effective method that can be used to select the best number of principal components for the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16969b58610>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train_p = pca.fit_transform(X_train)\n",
    "X_test_p  = pca.transform(X_test)\n",
    "\n",
    "exp_var = pca.explained_variance_ratio_ * 100\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "plt.bar(range(1, 5), exp_var, align='center',\n",
    "        label='Individual explained variance')\n",
    "\n",
    "plt.step(range(1, 5), cum_exp_var, where='mid',\n",
    "         label='Cumulative explained variance', color='red')\n",
    "\n",
    "plt.ylabel('Explained variance percentage')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.xticks(ticks=[1, 2, 3, 4])\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of bars is equal to the number of variables in the original dataset. In this plot, each bar shows the explained variance percentage of individual components and the step plot shows the cumulative explained variance percentages.\n",
    "\n",
    "By looking at this plot, we can easily decide how many components should be kept. In this example, only the first two components capture almost all the variance in the dataset. So, we decide to select only the first two components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to use 1 principal component to train our algorithm. To do so, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_train_p1 = pca.fit_transform(X_train)\n",
    "X_test_p1  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using random forest classification for making the predictions.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train_p1, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  1  5]]\n",
      "Accuracy:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# performance evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the output that with only one feature, the random forest algorithm is able to correctly predict 28 out of 30 instances, resulting in 93.33% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  9  4]\n",
      " [ 0  2  4]]\n",
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_p2 = pca.fit_transform(X_train)\n",
    "X_test_p2  = pca.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train_p2, y_train)\n",
    "y_pred = clf.predict(X_test_p2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the number of components for PCA has been set to 2. The classification results with 2 components are given above:\n",
    "With two principal components the classification accuracy decreases to 80% compared to 93.33% for 1 component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  8  5]\n",
      " [ 0  1  5]]\n",
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_train_p3 = pca.fit_transform(X_train)\n",
    "X_test_p3  = pca.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train_p3, y_train)\n",
    "y_pred = clf.predict(X_test_p3)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the number of components for PCA has been set to 3. The classification results with 2 components are given above:\n",
    "With two principal components the classification accuracy decreases to 80% compared to 93.33% for 1 component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with Full Feature Set\n",
    "Let's try to find the results with full feature set. To do so, simply remove the PCA part from the script that we wrote above. The results with full feature set, without applying PCA looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "From the above experimentation we achieved optimal level of accuracy while significantly reducing the number of features in the dataset. We saw that accuracy achieved with only 1 principal component is not bad when comparing the accuracy achieved with will feature set i.e. 100%. It is also pertinent to mention that the accuracy of a classifier doesn't necessarily improve with increased number of principal components. From the results we can see that the accuracy achieved with one principal component (93.33%) was greater than the one achieved with two principal components (80%).\n",
    "\n",
    "The number of principal components to retain in a feature set depends on several conditions such as storage capacity, training time, performance, etc. In some dataset all the features are contributing equally to the overall variance, therefore all the principal components are crucial to the predictions and none can be ignored. A general rule of thumb is to take number of principal of principal components that contribute to significant variance and ignore those with diminishing variance returns. A good way is to plot the variance against principal components and ignore the principal components with diminishing values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd275371c28e8315e03adf8b037f110cb1b8ea1df403d06bca1b2c33c8fa2b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
